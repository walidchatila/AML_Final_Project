{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/walid/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tags(path, cat = False):\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as tag_file:           \n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            if cat == True:\n",
    "                sentence = \" \".join(\" \".join(line.strip() for line in tag_file).split(\":\"))\n",
    "            else:\n",
    "                sentence = (line.strip() for line in tag_file)\n",
    "                sentence = (\" \".join(\" \".join(word.split(\":\")[1:]) for word in sentence))\n",
    "            new_row = (sentence, label) \n",
    "            data.append(new_row)\n",
    "            \n",
    "\n",
    "    dt = np.dtype([('sentence', object), ('label', 'int64')])\n",
    "    return(np.array(data, dtype = dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags_path = \"data/tags_train/\"\n",
    "test_tags_path = \"data/tags_test/\"\n",
    "\n",
    "### Tags with categories\n",
    "raw_train_tags = load_tags(train_tags_path, cat = True)\n",
    "raw_test_tags = load_tags(test_tags_path, cat = True)\n",
    "\n",
    "### Tags without categories\n",
    "raw_train_tags_nocat = load_tags(train_tags_path)\n",
    "raw_test_tags_nocat = load_tags(test_tags_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('vehicle airplane outdoor bench sports skateboard person person vehicle truck accessory backpack accessory handbag furniture dining table',    0)\n",
      " ('kitchen bowl food carrot kitchen spoon',    1)\n",
      " ('accessory suitcase',   10) ('food cake',  100)\n",
      " ('outdoor traffic light', 1000) ('animal cat', 1001)\n",
      " ('vehicle airplane person person', 1002)\n",
      " ('vehicle car person person sports skis accessory handbag outdoor traffic light', 1003)\n",
      " ('person person electronic remote furniture couch', 1004)\n",
      " ('vehicle boat person person animal bird', 1005)]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_tags.shape, raw_test_tags.shape)\n",
    "print(raw_train_tags[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('airplane bench skateboard person truck backpack handbag dining table',    0)\n",
      " ('bowl carrot spoon',    1) ('suitcase',   10) ('cake',  100)\n",
      " ('traffic light', 1000) ('cat', 1001) ('airplane person', 1002)\n",
      " ('car person skis handbag traffic light', 1003)\n",
      " ('person remote couch', 1004) ('boat person bird', 1005)]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_tags_nocat.shape, raw_test_tags_nocat.shape)\n",
    "print(raw_train_tags_nocat[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_tags.sort(order = 'label', axis = 0)\n",
    "raw_test_tags.sort(order = 'label', axis = 0)\n",
    "\n",
    "raw_train_tags_nocat.sort(order = 'label', axis = 0)\n",
    "raw_test_tags_nocat.sort(order = 'label', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vehicle airplane outdoor bench sports skateboard person person vehicle truck accessory backpack accessory handbag furniture dining table', 0)\n",
      " ('kitchen bowl food carrot kitchen spoon', 1)\n",
      " ('vehicle car vehicle truck outdoor traffic light person person', 2)\n",
      " ('person person outdoor bench sports frisbee vehicle car', 3)\n",
      " ('person person sports baseball bat', 4)\n",
      " ('furniture bed furniture chair electronic mouse electronic keyboard indoor book kitchen cup electronic tv electronic laptop', 5)\n",
      " ('person person food donut vehicle bicycle', 6)\n",
      " ('person person accessory tie', 7) ('vehicle car person person', 8)\n",
      " ('vehicle car vehicle bus accessory backpack', 9)]\n",
      "[('airplane bench skateboard person truck backpack handbag dining table', 0)\n",
      " ('bowl carrot spoon', 1) ('car truck traffic light person', 2)\n",
      " ('person bench frisbee car', 3) ('person baseball bat', 4)\n",
      " ('bed chair mouse keyboard book cup tv laptop', 5)\n",
      " ('person donut bicycle', 6) ('person tie', 7) ('car person', 8)\n",
      " ('car bus backpack', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_tags[0:10])\n",
    "print(raw_train_tags_nocat[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keep_pos_nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "keep_pos_all = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBG', 'VBZ']\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(data, pos_to_keep = None, lemma = False):\n",
    "    \n",
    "    out = data.copy()\n",
    "    out['sentence'] = list(map(lambda x:x.lower(), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:''.join(ch for ch in x if ch not in punctuation), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:' '.join(w for w in x.split(' ') if w not in stop_words), out['sentence']))\n",
    "    if lemma == True:\n",
    "        out['sentence'] = list(map(lambda x:' '.join(wnl.lemmatize(w) for w in x.split(' ')), out['sentence']))\n",
    "    \n",
    "    if pos_to_keep != None:\n",
    "        out['sentence'] = list(map(lambda x:' '.join(w[0] for w in nltk.pos_tag(nltk.word_tokenize(x)) if w[1] in pos_to_keep), out['sentence']))\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_all = preprocess(raw_train_tags, lemma = True)\n",
    "test_tag_all = preprocess(raw_test_tags, lemma = True)\n",
    "\n",
    "train_tag = preprocess(raw_train_tags, keep_pos_all, lemma = True)\n",
    "test_tag =  preprocess(raw_test_tags, keep_pos_all, lemma = True)\n",
    "\n",
    "train_tag_nocat = preprocess(raw_train_tags_nocat, keep_pos_all, lemma = True)\n",
    "test_tag_nocat =  preprocess(raw_test_tags_nocat, keep_pos_all, lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_tag_all_noLemma = preprocess(raw_train_tags)\n",
    "test_tag_all_noLemma = preprocess(raw_test_tags)\n",
    "\n",
    "train_tag_noLemma = preprocess(raw_train_tags, keep_pos_all)\n",
    "test_tag_noLemma =  preprocess(raw_test_tags, keep_pos_all)\n",
    "\n",
    "train_tag_nocat_noLemma = preprocess(raw_train_tags_nocat, keep_pos_all)\n",
    "test_tag_nocat_noLemma =  preprocess(raw_test_tags_nocat, keep_pos_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_data, test_data, binary = False):\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(vocabulary = tag_dict, binary = binary,\n",
    "                            tokenizer = lambda str: str.split(\" \"))\n",
    "    \n",
    "    tr_d = [word for word in train_data['sentence'].tolist()] \n",
    "    te_d = [word for word in test_data['sentence'].tolist()] \n",
    "    \n",
    "    Y_train = Tfidf.fit_transform(tr_d)\n",
    "    Y_test = Tfidf.fit_transform(te_d)\n",
    "    \n",
    "    return(Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accessory',\n",
       " 'airplane',\n",
       " 'animal',\n",
       " 'apple',\n",
       " 'appliance',\n",
       " 'backpack',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dict = set()\n",
    "for s in train_tag['sentence']:\n",
    "    tags = s.split()\n",
    "    for tag in tags:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "print(len(tag_dict))\n",
    "tag_dict_list = list(tag_dict)\n",
    "tag_dict_list.sort()\n",
    "tag_dict_list[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 101) (10000, 101)\n"
     ]
    }
   ],
   "source": [
    "### BOW with categories \n",
    "\n",
    "Y_train, Y_test = vectorize(train_tag, test_tag)\n",
    "Y_train_binary, Y_test_binary = vectorize(train_tag, test_tag, binary=True)\n",
    "\n",
    "print(Y_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'apple',\n",
       " 'backpack',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli',\n",
       " 'bus',\n",
       " 'cake',\n",
       " 'car']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BOW without categories \n",
    "\n",
    "_tag_dict_list = tag_dict_list\n",
    "tag_dict = set()\n",
    "for s in train_tag_nocat['sentence']:\n",
    "    tags = s.split()\n",
    "    for tag in tags:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "print(len(tag_dict))\n",
    "tag_dict_list = list(tag_dict)\n",
    "tag_dict_list.sort()\n",
    "tag_dict_list[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 92) (2000, 92)\n"
     ]
    }
   ],
   "source": [
    "### BOW without categories \n",
    "\n",
    "Y_train_nocat, Y_test_nocat = vectorize(train_tag_nocat, test_tag_nocat)\n",
    "Y_train_nocat_binary, Y_test_nocat_binary = vectorize(train_tag_nocat, test_tag_nocat, binary=True)\n",
    "\n",
    "print(Y_train_nocat.shape, Y_test_nocat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accessory',\n",
       " 'airplane',\n",
       " 'animal',\n",
       " 'apple',\n",
       " 'appliance',\n",
       " 'backpack',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BOW noLemma\n",
    "\n",
    "tag_dict = set()\n",
    "for s in train_tag_noLemma['sentence']:\n",
    "    tags = s.split()\n",
    "    for tag in tags:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "print(len(tag_dict))\n",
    "tag_noLemma_dict_list = list(tag_dict)\n",
    "tag_noLemma_dict_list.sort()\n",
    "tag_noLemma_dict_list[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 101) (10000, 101)\n"
     ]
    }
   ],
   "source": [
    "### BOW with categories no Lemmatization\n",
    "\n",
    "Y_train_noLemma, Y_test_noLemma = vectorize(train_tag_noLemma, test_tag_noLemma)\n",
    "Y_train_binary_noLemma, Y_test_binary_noLemma = vectorize(train_tag_noLemma, test_tag_noLemma, binary=True)\n",
    "\n",
    "print(Y_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'apple',\n",
       " 'backpack',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli',\n",
       " 'bus',\n",
       " 'cake',\n",
       " 'car']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BOW without categories \n",
    "\n",
    "_tag_noLemma_dict_list = tag_noLemma_dict_list\n",
    "tag_dict = set()\n",
    "for s in train_tag_nocat_noLemma['sentence']:\n",
    "    tags = s.split()\n",
    "    for tag in tags:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "print(len(tag_dict))\n",
    "tag_noLemma_dict_list = list(tag_dict)\n",
    "tag_noLemma_dict_list.sort()\n",
    "tag_noLemma_dict_list[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 92) (2000, 92)\n"
     ]
    }
   ],
   "source": [
    "### BOW without categories no lemmatization\n",
    "\n",
    "Y_train_nocat_noLemma, Y_test_nocat_noLemma = vectorize(train_tag_nocat_noLemma, test_tag_nocat_noLemma)\n",
    "Y_train_nocat_binary_noLemma, Y_test_nocat_binary_noLemma = vectorize(train_tag_nocat_noLemma, test_tag_nocat_noLemma, binary=True)\n",
    "\n",
    "print(Y_train_nocat.shape, Y_test_nocat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_tags\"):\n",
    "    os.mkdir(\"data/processed_tags\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_tfdif.npz\", Y_train)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_tfdif.npz\", Y_test)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_nocat_tfdif.npz\", Y_train_nocat)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_nocat_tfdif.npz\", Y_test_nocat)\n",
    "\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_tfdif_binary.npz\", Y_train_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_tfdif_binary.npz\", Y_test_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_nocat_tfdif_binary.npz\", Y_train_nocat_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_nocat_tfdif_binary.npz\", Y_test_nocat_binary)\n",
    "\n",
    "with open('data/processed_tags/tag_list.txt', 'w') as f:\n",
    "    for item in tag_dict_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('data/processed_tags/tag_list_.txt', 'w') as f:\n",
    "    for item in _tag_dict_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_tags\"):\n",
    "    os.mkdir(\"data/processed_tags\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_tfdif_noLemma.npz\", Y_train_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_tfdif_noLemma.npz\", Y_test_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_nocat_tfdif_noLemma.npz\", Y_train_nocat_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_nocat_tfdif_noLemma.npz\", Y_test_nocat_noLemma)\n",
    "\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_tfdif_binary_noLemma.npz\", Y_train_binary_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_tfdif_binary_noLemma.npz\", Y_test_binary_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_nocat_tfdif_binary_noLemma.npz\", Y_train_nocat_binary_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_nocat_tfdif_binary_noLemma.npz\", Y_test_nocat_binary_noLemma)\n",
    "\n",
    "with open('data/processed_tags/tag_noLemma_list.txt', 'w') as f:\n",
    "    for item in tag_noLemma_dict_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('data/processed_tags/tag_noLemma_list_.txt', 'w') as f:\n",
    "    for item in _tag_noLemma_dict_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
