{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/walid/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tags(path, cat = False):\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as tag_file:           \n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            if cat == True:\n",
    "                sentence = \" \".join(\" \".join(line.strip() for line in tag_file).split(\":\"))\n",
    "            else:\n",
    "                sentence = (line.strip() for line in tag_file)\n",
    "                sentence = (\" \".join(\" \".join(word.split(\":\")[1:]) for word in sentence))\n",
    "            new_row = (sentence, label) \n",
    "            data.append(new_row)\n",
    "            \n",
    "\n",
    "    dt = np.dtype([('sentence', object), ('label', object)])\n",
    "    return(np.array(data, dtype = dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags_path = \"data/tags_train/\"\n",
    "test_tags_path = \"data/tags_test/\"\n",
    "\n",
    "### Tags with categories\n",
    "raw_train_tags = load_tags(train_tags_path, cat = True)\n",
    "raw_test_tags = load_tags(test_tags_path, cat = True)\n",
    "\n",
    "### Tags without categories\n",
    "raw_train_tags_nocat = load_tags(train_tags_path)\n",
    "raw_test_tags_nocat = load_tags(test_tags_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('vehicle airplane outdoor bench sports skateboard person person vehicle truck accessory backpack accessory handbag furniture dining table', '0')\n",
      " ('kitchen bowl food carrot kitchen spoon', '1')\n",
      " ('accessory suitcase', '10') ('food cake', '100')\n",
      " ('outdoor traffic light', '1000') ('animal cat', '1001')\n",
      " ('vehicle airplane person person', '1002')\n",
      " ('vehicle car person person sports skis accessory handbag outdoor traffic light', '1003')\n",
      " ('person person electronic remote furniture couch', '1004')\n",
      " ('vehicle boat person person animal bird', '1005')]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_tags.shape, raw_test_tags.shape)\n",
    "print(raw_train_tags[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('airplane bench skateboard person truck backpack handbag dining table', '0')\n",
      " ('bowl carrot spoon', '1') ('suitcase', '10') ('cake', '100')\n",
      " ('traffic light', '1000') ('cat', '1001') ('airplane person', '1002')\n",
      " ('car person skis handbag traffic light', '1003')\n",
      " ('person remote couch', '1004') ('boat person bird', '1005')]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_tags_nocat.shape, raw_test_tags_nocat.shape)\n",
    "print(raw_train_tags_nocat[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keep_pos_nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "keep_pos_all = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBG', 'VBZ']\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(data, pos_to_keep = None):\n",
    "    \n",
    "    out = data.copy()\n",
    "    out['sentence'] = list(map(lambda x:x.lower(), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:''.join(ch for ch in x if ch not in punctuation), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:' '.join(w for w in x.split(' ') if w not in stop_words), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:' '.join(wnl.lemmatize(w) for w in x.split(' ')), out['sentence']))\n",
    "    \n",
    "    if pos_to_keep != None:\n",
    "        out['sentence'] = list(map(lambda x:' '.join(w[0] for w in nltk.pos_tag(nltk.word_tokenize(x)) if w[1] in pos_to_keep), out['sentence']))\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_all = preprocess(raw_train_tags)\n",
    "test_tag_all = preprocess(raw_test_tags)\n",
    "\n",
    "train_tag = preprocess(raw_train_tags, keep_pos_all)\n",
    "test_tag =  preprocess(raw_test_tags, keep_pos_all)\n",
    "\n",
    "train_tag_nocat = preprocess(raw_train_tags_nocat, keep_pos_all)\n",
    "test_tag_nocat =  preprocess(raw_test_tags_nocat, keep_pos_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_data, test_data):\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(vocabulary = tag_dict, \n",
    "                            tokenizer = lambda str: str.split(\" \"))\n",
    "    \n",
    "    tr_d = [word for word in train_data['sentence'].tolist()] \n",
    "    te_d = [word for word in test_data['sentence'].tolist()] \n",
    "    \n",
    "    Y_train = Tfidf.fit_transform(tr_d)\n",
    "    Y_test = Tfidf.fit_transform(te_d)\n",
    "    \n",
    "    return(Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accessory',\n",
       " 'airplane',\n",
       " 'animal',\n",
       " 'apple',\n",
       " 'appliance',\n",
       " 'backpack',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dict = set()\n",
    "for s in train_tag['sentence']:\n",
    "    tags = s.split()\n",
    "    for tag in tags:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "print(len(tag_dict))\n",
    "tag_dict_list = list(tag_dict)\n",
    "tag_dict_list.sort()\n",
    "tag_dict_list[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 101) (10000, 101)\n"
     ]
    }
   ],
   "source": [
    "### BOW with categories \n",
    "\n",
    "Y_train, Y_test = vectorize(train_tag, test_tag)\n",
    "print(Y_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "  (0, 98)\t0.34534438667719125\n",
      "  (0, 94)\t0.2815251107733758\n",
      "  (0, 85)\t0.2354923920328581\n",
      "  (0, 81)\t0.18676698136694395\n",
      "  (0, 65)\t0.22974712352115403\n",
      "  (0, 62)\t0.22925214756478446\n",
      "  (0, 46)\t0.26879722504279113\n",
      "  (0, 41)\t0.17033679166940313\n",
      "  (0, 31)\t0.2354923920328581\n",
      "  (0, 12)\t0.284842935230572\n",
      "  (0, 5)\t0.2921512902238589\n",
      "  (0, 1)\t0.3396880170215032\n",
      "  (0, 0)\t0.40890456845147044\n",
      "test:\n",
      "  (0, 88)\t0.37188894156615054\n",
      "  (0, 83)\t0.41059946675486214\n",
      "  (0, 41)\t0.20202717835040415\n",
      "  (0, 5)\t0.33403775326719826\n",
      "  (0, 0)\t0.7353301922547499\n"
     ]
    }
   ],
   "source": [
    "### BOW with categories \n",
    "\n",
    "print(\"train:\")\n",
    "print(Y_train[0])\n",
    "print(\"test:\")\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'apple',\n",
       " 'backpack',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'bench',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'broccoli',\n",
       " 'bus',\n",
       " 'cake',\n",
       " 'car']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BOW without categories \n",
    "\n",
    "_tag_dict_list = tag_dict_list\n",
    "tag_dict = set()\n",
    "for s in train_tag_nocat['sentence']:\n",
    "    tags = s.split()\n",
    "    for tag in tags:\n",
    "        tag_dict.add(tag)\n",
    "        \n",
    "print(len(tag_dict))\n",
    "tag_dict_list = list(tag_dict)\n",
    "tag_dict_list.sort()\n",
    "tag_dict_list[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accessory', 'animal', 'appliance', 'electronic', 'food', 'furniture', 'indoor', 'kitchen', 'outdoor', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "print([t for t in _tag_dict_list if t not in tag_dict_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 92) (2000, 92)\n"
     ]
    }
   ],
   "source": [
    "### BOW without categories \n",
    "\n",
    "Y_train_nocat, Y_test_nocat = vectorize(train_tag_nocat, test_tag_nocat)\n",
    "print(Y_train_nocat.shape, Y_test_nocat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "  (0, 86)\t0.37611893193939083\n",
      "  (0, 77)\t0.3146189934103292\n",
      "  (0, 56)\t0.15347164321779547\n",
      "  (0, 40)\t0.36612630542609204\n",
      "  (0, 28)\t0.3146189934103292\n",
      "  (0, 9)\t0.38055156172426136\n",
      "  (0, 2)\t0.39208327023739353\n",
      "  (0, 0)\t0.45212114336752823\n",
      "test:\n",
      "  (0, 80)\t0.493714458737659\n",
      "  (0, 75)\t0.5412847965145303\n",
      "  (0, 8)\t0.5171151075938532\n",
      "  (0, 2)\t0.4425480400971191\n"
     ]
    }
   ],
   "source": [
    "### BOW without categories \n",
    "\n",
    "print(\"train:\")\n",
    "print(Y_train_nocat[0])\n",
    "print(\"test:\")\n",
    "print(Y_test_nocat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_dict_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0300d20eb9f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/processed_tags/test_tag_nocat_tfdif.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_nocat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/processed_descriptions/word_list.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_dict_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_dict_list' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"data/processed_tags\"):\n",
    "    os.mkdir(\"data/processed_tags\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_tfdif.npz\", Y_train)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_tfdif.npz\", Y_test)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/train_tag_nocat_tfdif.npz\", Y_train_nocat)\n",
    "scipy.sparse.save_npz(\"data/processed_tags/test_tag_nocat_tfdif.npz\", Y_test_nocat)\n",
    "with open('data/processed_tags/tag_list.txt', 'w') as f:\n",
    "    for item in _tag_dict_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('data/processed_tags/tag_list_.txt', 'w') as f:\n",
    "    for item in _tag_dict_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
