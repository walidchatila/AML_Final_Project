{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/walid/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000) (2000, 1000)\n",
      "(10000, 2048) (2000, 2048)\n"
     ]
    }
   ],
   "source": [
    "resnet_fc1000_train = pd.read_csv(\"data/features_train/features_resnet1000_train.csv\", header = None, index_col = 0)\n",
    "resnet_train = pd.read_csv(\"data/features_train/features_resnet1000intermediate_train.csv\", header = None, index_col = 0)\n",
    "\n",
    "resnet_fc1000_test = pd.read_csv(\"data/features_test/features_resnet1000_test.csv\", header = None, index_col = 0)\n",
    "resnet_test = pd.read_csv(\"data/features_test/features_resnet1000intermediate_test.csv\", header = None, index_col = 0)\n",
    "\n",
    "print(resnet_fc1000_train.shape, resnet_fc1000_test.shape)\n",
    "print(resnet_train.shape, resnet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           1         2         3         4         5     \\\n",
      "0                                                                         \n",
      "images_train/5373.jpg  0.460352  1.018530  0.283533  0.053866  1.071661   \n",
      "images_train/984.jpg   1.498549  0.295754  0.266695  0.265754  0.793800   \n",
      "images_train/7127.jpg  0.460160  0.570405  0.535643  0.073016  0.158414   \n",
      "images_train/5293.jpg  0.626306  0.095554  0.175943  0.096139  0.063999   \n",
      "images_train/3688.jpg  0.253012  1.743208  0.359000  0.486566  0.473813   \n",
      "images_train/3340.jpg  0.920477  0.506810  0.334729  0.551220  0.201660   \n",
      "images_train/4787.jpg  0.312581  2.718709  0.456360  0.133989  0.071897   \n",
      "images_train/5707.jpg  0.992423  0.185353  0.040412  0.651217  0.514700   \n",
      "images_train/1262.jpg  0.504020  1.136837  0.122905  0.255936  0.000000   \n",
      "images_train/8355.jpg  0.153254  0.353655  0.475493  0.386336  0.167553   \n",
      "\n",
      "                           6         7         8         9         10    \\\n",
      "0                                                                         \n",
      "images_train/5373.jpg  0.017668  0.330733  0.092461  0.611386  0.747699   \n",
      "images_train/984.jpg   0.115943  0.237539  0.612437  1.229246  0.395554   \n",
      "images_train/7127.jpg  0.391194  0.088051  1.249013  0.032401  0.637476   \n",
      "images_train/5293.jpg  0.761737  0.107138  0.120813  1.628744  0.790666   \n",
      "images_train/3688.jpg  0.141680  0.164890  0.533619  0.268889  0.026012   \n",
      "images_train/3340.jpg  0.355633  0.405342  0.096502  0.438416  0.191141   \n",
      "images_train/4787.jpg  0.111682  0.085940  0.926424  0.409320  0.140058   \n",
      "images_train/5707.jpg  0.416380  0.376781  0.066802  0.934469  0.085382   \n",
      "images_train/1262.jpg  0.013570  0.100736  1.323465  0.287882  0.190674   \n",
      "images_train/8355.jpg  0.404289  0.274328  0.239030  0.572713  0.167199   \n",
      "\n",
      "                         ...         2039      2040      2041      2042  \\\n",
      "0                        ...                                              \n",
      "images_train/5373.jpg    ...     0.164259  0.422783  0.592163  0.349664   \n",
      "images_train/984.jpg     ...     0.421009  0.462610  0.552496  0.138283   \n",
      "images_train/7127.jpg    ...     0.232817  0.338647  0.925935  0.150575   \n",
      "images_train/5293.jpg    ...     0.157466  0.148046  1.304605  0.556068   \n",
      "images_train/3688.jpg    ...     0.317842  0.260543  0.374608  0.043875   \n",
      "images_train/3340.jpg    ...     0.894199  0.474975  0.402254  0.433358   \n",
      "images_train/4787.jpg    ...     0.622792  0.571353  0.263866  0.078321   \n",
      "images_train/5707.jpg    ...     0.633105  0.264808  0.068898  0.106413   \n",
      "images_train/1262.jpg    ...     0.501629  0.656466  0.058838  0.479665   \n",
      "images_train/8355.jpg    ...     0.438768  0.267500  0.867514  0.851634   \n",
      "\n",
      "                           2043      2044      2045      2046      2047  \\\n",
      "0                                                                         \n",
      "images_train/5373.jpg  0.130692  0.245438  0.021640  0.412349  0.443392   \n",
      "images_train/984.jpg   0.205361  0.343715  0.160027  0.172050  0.106674   \n",
      "images_train/7127.jpg  0.220055  0.008997  0.135737  0.325504  0.006667   \n",
      "images_train/5293.jpg  0.032862  0.906909  0.105638  0.230118  0.785661   \n",
      "images_train/3688.jpg  0.453899  0.265805  0.106244  0.038468  0.539088   \n",
      "images_train/3340.jpg  0.481877  1.647253  0.089888  0.447249  0.699559   \n",
      "images_train/4787.jpg  0.016421  0.336534  0.209327  0.769715  0.387314   \n",
      "images_train/5707.jpg  0.092712  0.130149  0.020998  0.091447  0.629647   \n",
      "images_train/1262.jpg  0.218414  0.356702  0.486234  0.117338  0.691074   \n",
      "images_train/8355.jpg  0.353608  0.261028  0.438352  0.152090  0.223081   \n",
      "\n",
      "                           2048  \n",
      "0                                \n",
      "images_train/5373.jpg  0.035718  \n",
      "images_train/984.jpg   0.054805  \n",
      "images_train/7127.jpg  0.432770  \n",
      "images_train/5293.jpg  0.175187  \n",
      "images_train/3688.jpg  0.176292  \n",
      "images_train/3340.jpg  0.358438  \n",
      "images_train/4787.jpg  1.169359  \n",
      "images_train/5707.jpg  0.222146  \n",
      "images_train/1262.jpg  1.049260  \n",
      "images_train/8355.jpg  0.177542  \n",
      "\n",
      "[10 rows x 2048 columns]\n"
     ]
    }
   ],
   "source": [
    "print(resnet_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(path):\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as description_file:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            sentence = \" \".join(line.strip() for line in description_file)\n",
    "            new_row = (sentence, label) \n",
    "            data.append(new_row)\n",
    "\n",
    "    dt = np.dtype([('sentence', object), ('label', object)])\n",
    "    return(np.array(data, dtype = dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions_path = \"data/descriptions_train/\"\n",
    "test_descriptions_path = \"data/descriptions_test/\"\n",
    "\n",
    "raw_train_data = load_descriptions(train_descriptions_path)\n",
    "raw_test_data = load_descriptions(test_descriptions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('The skateboarder is putting on a show using the picnic table as his stage. A skateboarder pulling tricks on top of a picnic table. A man riding on a skateboard on top of a table. A skate boarder doing a trick on a picnic table. A person is riding a skateboard on a picnic table with a crowd watching.', '0')\n",
      " ('A bowl of soup that has some carrots, shrimp, and noodles in it. The healthy food is in the bowl and ready to eat. Soup has carrots and shrimp in it as it sits next to chopsticks. A tasty bowl of ramen is served for someone to enjoy. Bowl of Asian noodle soup, with shrimp and carrots.', '1')\n",
      " ('A bunch of luggage laying on an area rug. Several pieces of luggage on a floor with an area rug. The luggage is sitting on top of the persian rug. a bunch of travel bags sit on a carpet floor Several pieces of luggage that are laying on the floor.', '10')\n",
      " ('The browned cracked crust of a baked berry pie. A brown crust of pie with strawberry filing. The top of a pie looks all crusty and good. A close up of a cooked fruit flavored muffin. An image of some sort of strawberry flaky pastery on display', '100')\n",
      " ('The sign is in clear view by the traffic lights. A Juice Theory sign is near a red traffic light. The bright sign is hanging next to the traffic signal. A sign on a electrical post that is about juice. A sign reading \"Juice Theory\" near a traffic light.', '1000')\n",
      " ('A cat is  sleeping inside a duffel bag. An orange cat lays in a black duffel bag. A cat sitting inside of a bag by a curtain. a small dog is packes into a duffel bag a travel bag with a cat laying inside of it', '1001')\n",
      " ('Two men walking through a field next to a large jet liner. Two people walking through a field as a plane lands. people walking down a path as a plane lands on a runway Two men are standing in a field near a landing strip. Two men walking a dog and watching an airplane about to take off.', '1002')\n",
      " ('A group of people riding skis down a snow covered street. A family skiing a city street while others clean snow off their cars. People are riding on skis in the snow on a street. Several people going down a snowy street in skis. The people have there skis on in the middle of the street.', '1003')\n",
      " ('A couple of guys standing up and playing Wii together. Adult men actively play video games with Wii controllers. A group of young men are playing a video game. Two men standing in a living room holding Wii remotes and nun-chucks. A couple of men standing in a living room together.', '1004')\n",
      " ('The boats navigated around the curves of the attraction. A few empty boats at a river ride Small boats sit unused in water by a dock. Different color boats along a river in a water park. A multicolored boat ride at an amusement park.', '1005')]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_data.shape, raw_test_data.shape)\n",
    "print(raw_train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keep_pos_nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "keep_pos_all = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBG', 'VBZ']\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(data, pos_to_keep = None):\n",
    "    \n",
    "    out = data.copy()\n",
    "    out['sentence'] = list(map(lambda x:x.lower(), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:''.join(ch for ch in x if ch not in punctuation), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:' '.join(w for w in x.split(' ') if w not in stop_words), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:' '.join(wnl.lemmatize(w) for w in x.split(' ')), out['sentence']))\n",
    "    \n",
    "    if pos_to_keep != None:\n",
    "        out['sentence'] = list(map(lambda x:' '.join(w[0] for w in nltk.pos_tag(nltk.word_tokenize(x)) if w[1] in pos_to_keep), out['sentence']))\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(raw_train_data, keep_pos_all)\n",
    "test_data =  preprocess(raw_test_data, keep_pos_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('skateboarder putting show using picnic table stage skateboarder pulling trick top picnic table man riding skateboard top table skate boarder trick picnic table person riding skateboard picnic table crowd watching', '0')\n",
      " ('bowl soup carrot shrimp noodle healthy food bowl ready eat soup carrot shrimp sits chopstick tasty bowl ramen someone enjoy bowl asian noodle soup shrimp carrot', '1')\n",
      " ('bunch luggage laying area several piece luggage floor area rug luggage sitting top persian rug bunch travel bag sit carpet floor several piece luggage laying floor', '10')\n",
      " ('cracked crust berry brown crust pie strawberry filing top pie look good cooked fruit muffin image sort flaky pastery display', '100')\n",
      " ('sign clear view traffic light juice theory sign red traffic light bright sign hanging next traffic signal sign electrical post juice sign reading juice theory traffic light', '1000')\n",
      " ('cat sleeping duffel bag orange cat black duffel bag cat sitting bag small dog packes bag travel bag cat laying', '1001')\n",
      " ('men walking field next large jet liner people walking field plane land people walking path plane men standing field landing strip men walking dog watching airplane take', '1002')\n",
      " ('group people riding ski snow street family skiing city street others snow car people riding ski snow street several people going snowy street ski people middle street', '1003')\n",
      " ('couple guy standing playing wii adult men video game wii controller group young men playing video game men standing living room holding wii remote nunchucks men standing living room', '1004')\n",
      " ('boat curve attraction empty boat river small boat sit unused water dock different color boat river water park boat ride amusement park', '1005')]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = set()\n",
    "for s in train_data['sentence']:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        word_dict.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'abandoned',\n",
       " 'abdomen',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abll',\n",
       " 'abnormal',\n",
       " 'aboard',\n",
       " 'abook',\n",
       " 'abou',\n",
       " 'abraham',\n",
       " 'abreast',\n",
       " 'abstract',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abyss',\n",
       " 'acacia',\n",
       " 'academic',\n",
       " 'accelerates',\n",
       " 'accent']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_dict))\n",
    "word_dict_list = list(word_dict)\n",
    "word_dict_list.sort()\n",
    "word_dict_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_data, test_data):\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(vocabulary = word_dict, \n",
    "                            tokenizer = lambda str: str.split(\" \"))\n",
    "    \n",
    "    tr_d = [word for word in train_data['sentence'].tolist()] \n",
    "    te_d = [word for word in test_data['sentence'].tolist()] \n",
    "    \n",
    "    X_train = Tfidf.fit_transform(tr_d)\n",
    "    X_test = Tfidf.fit_transform(te_d)\n",
    "    \n",
    "    return(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7038) (2000, 7038)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = vectorize(train_data, test_data)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('skateboarder putting show using picnic table stage skateboarder pulling trick top picnic table man riding skateboard top table skate boarder trick picnic table person riding skateboard picnic table crowd watching', '0')\n",
      "  (0, 6783)\t0.12053926958581132\n",
      "  (0, 6619)\t0.12127189129739563\n",
      "  (0, 6414)\t0.24667824208973552\n",
      "  (0, 6319)\t0.14277759161319625\n",
      "  (0, 6091)\t0.3853043493472325\n",
      "  (0, 5802)\t0.18503232966603594\n",
      "  (0, 5522)\t0.26959294300622977\n",
      "  (0, 5521)\t0.23031291251512154\n",
      "  (0, 5519)\t0.1277550086027136\n",
      "  (0, 5433)\t0.13522916887226213\n",
      "  (0, 5027)\t0.17069388821130024\n",
      "  (0, 4796)\t0.15739505742327461\n",
      "  (0, 4772)\t0.13727613164596256\n",
      "  (0, 4449)\t0.6510591890903679\n",
      "  (0, 4412)\t0.07473838331697687\n",
      "  (0, 3575)\t0.06230190293115674\n",
      "  (0, 1490)\t0.12361014704073833\n",
      "  (0, 628)\t0.158491376463526\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picnic table skateboarder\n"
     ]
    }
   ],
   "source": [
    "print(word_dict_list[4449], word_dict_list[6091], word_dict_list[5522])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_descriptions\"):\n",
    "    os.mkdir(\"data/processed_descriptions\")\n",
    "\n",
    "np.save(\"data/processed_descriptions/train_descriptions.npy\", X_train)\n",
    "np.save(\"data/processed_descriptions/test_descriptions.npy\", X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
