{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/walid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/walid/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000) (2000, 1000)\n",
      "(10000, 2048) (2000, 2048)\n"
     ]
    }
   ],
   "source": [
    "resnet_fc1000_train = pd.read_csv(\"data/features_train/features_resnet1000_train.csv\", header = None, index_col = 0)\n",
    "resnet_train = pd.read_csv(\"data/features_train/features_resnet1000intermediate_train.csv\", header = None, index_col = 0)\n",
    "\n",
    "resnet_fc1000_test = pd.read_csv(\"data/features_test/features_resnet1000_test.csv\", header = None, index_col = 0)\n",
    "resnet_test = pd.read_csv(\"data/features_test/features_resnet1000intermediate_test.csv\", header = None, index_col = 0)\n",
    "\n",
    "print(resnet_fc1000_train.shape, resnet_fc1000_test.shape)\n",
    "print(resnet_train.shape, resnet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           1         2         3         4         5     \\\n",
      "0                                                                         \n",
      "images_train/5373.jpg -0.899450 -0.930470 -2.503365 -3.172499 -2.819133   \n",
      "images_train/984.jpg  -1.346954 -3.119461 -0.765971 -1.382550 -1.104675   \n",
      "images_train/7127.jpg -3.445498 -1.524573 -1.001654 -3.668335 -1.805517   \n",
      "images_train/9609.jpg  1.114650 -2.167102  0.097881 -1.336255  0.853483   \n",
      "images_train/5293.jpg  1.602650 -1.505817  3.029409  4.092412  1.711755   \n",
      "images_train/3688.jpg -1.598056 -3.197755 -1.103384 -0.969265 -0.756076   \n",
      "images_train/3340.jpg  3.967041 -2.200422 -0.272736  1.695661  2.074336   \n",
      "images_train/4787.jpg -1.349009 -0.148385 -1.723681 -2.070523 -2.659292   \n",
      "images_train/5707.jpg  0.676531 -1.743760  3.400268  2.481813  2.522729   \n",
      "images_train/1262.jpg -3.897769  0.144037 -3.280854 -3.507650 -0.995550   \n",
      "\n",
      "                           6         7         8         9         10    \\\n",
      "0                                                                         \n",
      "images_train/5373.jpg  0.992159 -3.698863  0.619991  0.956148 -0.218699   \n",
      "images_train/984.jpg  -3.656271 -4.815436 -0.556942 -1.402286  1.426897   \n",
      "images_train/7127.jpg -1.633496 -7.127826 -1.147802 -1.055816 -2.571585   \n",
      "images_train/9609.jpg -0.374885 -2.369090 -2.273191 -1.143788 -0.366391   \n",
      "images_train/5293.jpg  6.271253  4.173686 -2.177313  0.747789 -1.183095   \n",
      "images_train/3688.jpg -1.377843 -2.536916  0.959177 -1.190254 -2.460724   \n",
      "images_train/3340.jpg  5.443278  4.297240 -1.914581 -3.486772  2.134430   \n",
      "images_train/4787.jpg  1.233698 -3.030076 -1.315877 -0.603716 -2.460350   \n",
      "images_train/5707.jpg  1.446424  1.915292 -0.596382 -1.903387  4.129346   \n",
      "images_train/1262.jpg  1.209555 -4.043581 -1.906238 -1.083807 -0.539498   \n",
      "\n",
      "                         ...         991       992       993       994   \\\n",
      "0                        ...                                              \n",
      "images_train/5373.jpg    ...    -3.021916  2.214253 -1.382491  1.672911   \n",
      "images_train/984.jpg     ...     0.011003 -3.968805 -2.694711 -4.196480   \n",
      "images_train/7127.jpg    ...    -2.991777 -2.628053 -2.971074 -2.537039   \n",
      "images_train/9609.jpg    ...    -1.248134 -0.633126 -1.723514 -2.638832   \n",
      "images_train/5293.jpg    ...    -1.285806 -2.266481 -3.898053  2.295787   \n",
      "images_train/3688.jpg    ...    -1.462375 -3.780391  1.234524 -2.559528   \n",
      "images_train/3340.jpg    ...    -2.105626 -2.336287 -4.622218 -1.787587   \n",
      "images_train/4787.jpg    ...     0.256885 -4.363538 -2.709139 -2.792937   \n",
      "images_train/5707.jpg    ...    -2.186083 -1.293151 -3.159403 -0.634806   \n",
      "images_train/1262.jpg    ...    -3.188696 -3.889918 -4.361494 -1.736343   \n",
      "\n",
      "                           995       996       997       998       999   \\\n",
      "0                                                                         \n",
      "images_train/5373.jpg  1.014233  2.599949  2.773284 -2.066632  0.385754   \n",
      "images_train/984.jpg  -2.880234 -1.210742 -1.605143 -4.859987 -0.837670   \n",
      "images_train/7127.jpg -1.707429  1.013672  0.608460 -3.714998 -0.484735   \n",
      "images_train/9609.jpg  0.097149  4.647974  1.030138 -2.193836  1.044024   \n",
      "images_train/5293.jpg -1.749552  0.974188  1.258117 -1.975622 -1.278643   \n",
      "images_train/3688.jpg -1.496218 -0.378542  0.298074 -1.486927 -1.269319   \n",
      "images_train/3340.jpg -1.501848  0.808052  1.209267 -1.954793 -0.298671   \n",
      "images_train/4787.jpg -2.917552  0.342765  1.708725 -1.387726  1.374972   \n",
      "images_train/5707.jpg -1.410534  0.111361  0.235730 -2.909609 -0.303459   \n",
      "images_train/1262.jpg -3.492179 -0.567022  1.014016 -3.038599  0.311466   \n",
      "\n",
      "                           1000  \n",
      "0                                \n",
      "images_train/5373.jpg -3.241320  \n",
      "images_train/984.jpg  -0.967604  \n",
      "images_train/7127.jpg  0.138767  \n",
      "images_train/9609.jpg  0.176043  \n",
      "images_train/5293.jpg -1.941441  \n",
      "images_train/3688.jpg  0.295429  \n",
      "images_train/3340.jpg -2.184459  \n",
      "images_train/4787.jpg  0.366994  \n",
      "images_train/5707.jpg -2.668813  \n",
      "images_train/1262.jpg  5.743081  \n",
      "\n",
      "[10 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "print(resnet_fc1000_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['images_train/5373.jpg', 'images_train/984.jpg',\n",
      "       'images_train/7127.jpg', 'images_train/5293.jpg',\n",
      "       'images_train/3688.jpg', 'images_train/3340.jpg',\n",
      "       'images_train/4787.jpg', 'images_train/5707.jpg',\n",
      "       'images_train/1262.jpg', 'images_train/8355.jpg'],\n",
      "      dtype='object', name=0)\n",
      "[5373, 984, 7127, 5293, 3688, 3340, 4787, 5707, 1262, 8355]\n",
      "Index(['images_test/152.jpg', 'images_test/901.jpg', 'images_test/1609.jpg',\n",
      "       'images_test/501.jpg', 'images_test/517.jpg', 'images_test/1516.jpg',\n",
      "       'images_test/1822.jpg', 'images_test/1993.jpg', 'images_test/405.jpg',\n",
      "       'images_test/1718.jpg'],\n",
      "      dtype='object', name=0)\n",
      "[152, 901, 1609, 501, 517, 1516, 1822, 1993, 405, 1718]\n"
     ]
    }
   ],
   "source": [
    "tr_idx = resnet_train.index\n",
    "print(tr_idx[0:10])\n",
    "new_idx1 = [int(re.findall(\"\\d+\", idx)[0]) for idx in tr_idx]\n",
    "print(new_idx1[0:10])\n",
    "resnet_train = np.array(pd.DataFrame(np.array(resnet_train), index = new_idx1).sort_index())\n",
    "\n",
    "te_idx = resnet_test.index\n",
    "print(te_idx[0:10])\n",
    "new_idx2 = [int(re.findall(\"\\d+\", idx)[0]) for idx in te_idx]\n",
    "print(new_idx2[0:10])\n",
    "resnet_test = np.array(pd.DataFrame(np.array(resnet_test), index = new_idx2).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['images_train/5373.jpg', 'images_train/984.jpg',\n",
      "       'images_train/7127.jpg', 'images_train/9609.jpg',\n",
      "       'images_train/5293.jpg', 'images_train/3688.jpg',\n",
      "       'images_train/3340.jpg', 'images_train/4787.jpg',\n",
      "       'images_train/5707.jpg', 'images_train/1262.jpg'],\n",
      "      dtype='object', name=0)\n",
      "[5373, 984, 7127, 9609, 5293, 3688, 3340, 4787, 5707, 1262]\n",
      "Index(['images_test/152.jpg', 'images_test/901.jpg', 'images_test/1609.jpg',\n",
      "       'images_test/501.jpg', 'images_test/517.jpg', 'images_test/1822.jpg',\n",
      "       'images_test/1993.jpg', 'images_test/832.jpg', 'images_test/1462.jpg',\n",
      "       'images_test/1718.jpg'],\n",
      "      dtype='object', name=0)\n",
      "[152, 901, 1609, 501, 517, 1822, 1993, 832, 1462, 1718]\n"
     ]
    }
   ],
   "source": [
    "fc_tr_idx = resnet_fc1000_train.index\n",
    "print(fc_tr_idx[0:10])\n",
    "fc_new_idx1 = [int(re.findall(\"\\d+\", idx)[0]) for idx in fc_tr_idx]\n",
    "print(fc_new_idx1[0:10])\n",
    "resnet_fc1000_train = np.array(pd.DataFrame(np.array(resnet_fc1000_train), index = fc_new_idx1).sort_index())\n",
    "\n",
    "fc_te_idx = resnet_fc1000_test.index\n",
    "print(fc_te_idx[0:10])\n",
    "fc_new_idx2 = [int(re.findall(\"\\d+\", idx)[0]) for idx in fc_te_idx]\n",
    "print(fc_new_idx2[0:10])\n",
    "resnet_fc1000_test = np.array(pd.DataFrame(np.array(resnet_fc1000_test), index = fc_new_idx2).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03315091, 0.14875449, 0.04642047, ..., 0.07316806, 0.41725951,\n",
       "        0.19742125],\n",
       "       [0.23184368, 0.12003349, 0.09078766, ..., 0.27578694, 3.49657249,\n",
       "        0.06475818],\n",
       "       [0.62282479, 0.25091803, 0.68479973, ..., 0.46948081, 0.70399392,\n",
       "        0.23659612],\n",
       "       ...,\n",
       "       [0.56402719, 0.29538712, 0.06798451, ..., 0.0225115 , 0.31061313,\n",
       "        0.55103958],\n",
       "       [0.93449306, 1.23616803, 0.21350028, ..., 0.00760192, 0.04935887,\n",
       "        0.19552897],\n",
       "       [0.56998813, 1.18722653, 0.66700262, ..., 0.60558736, 0.43323767,\n",
       "        0.08355507]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.55291319, -3.33937263,  0.73980451, ..., -4.32532072,\n",
       "        -1.36100817, -0.1848945 ],\n",
       "       [ 1.05579996,  1.34591961, -4.62148428, ..., -0.33549014,\n",
       "         4.46938419, -0.19354703],\n",
       "       [-1.32305372, -1.99045908, -2.64625263, ..., -3.34685826,\n",
       "         0.924743  , -0.35301754],\n",
       "       ...,\n",
       "       [ 8.06348324, -0.69784027, -2.38401532, ...,  2.71859002,\n",
       "         2.58633995, -2.59397388],\n",
       "       [-3.03375983, -3.45618963, -1.92697704, ..., -4.45431709,\n",
       "        -0.95214069,  0.81716555],\n",
       "       [ 2.52032423, -2.25999498, -5.17780209, ..., -1.18195713,\n",
       "         2.06118369, -3.53105974]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_fc1000_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(path):\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as description_file:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            sentence = \" \".join(line.strip() for line in description_file)\n",
    "            new_row = (sentence, label) \n",
    "            data.append(new_row)\n",
    "\n",
    "    dt = np.dtype([('sentence', object), ('label', 'int64')])\n",
    "    return(np.array(data, dtype = dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions_path = \"data/descriptions_train/\"\n",
    "test_descriptions_path = \"data/descriptions_test/\"\n",
    "\n",
    "raw_train_data = load_descriptions(train_descriptions_path)\n",
    "raw_test_data = load_descriptions(test_descriptions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('The skateboarder is putting on a show using the picnic table as his stage. A skateboarder pulling tricks on top of a picnic table. A man riding on a skateboard on top of a table. A skate boarder doing a trick on a picnic table. A person is riding a skateboard on a picnic table with a crowd watching.',    0)\n",
      " ('A bowl of soup that has some carrots, shrimp, and noodles in it. The healthy food is in the bowl and ready to eat. Soup has carrots and shrimp in it as it sits next to chopsticks. A tasty bowl of ramen is served for someone to enjoy. Bowl of Asian noodle soup, with shrimp and carrots.',    1)\n",
      " ('A bunch of luggage laying on an area rug. Several pieces of luggage on a floor with an area rug. The luggage is sitting on top of the persian rug. a bunch of travel bags sit on a carpet floor Several pieces of luggage that are laying on the floor.',   10)\n",
      " ('The browned cracked crust of a baked berry pie. A brown crust of pie with strawberry filing. The top of a pie looks all crusty and good. A close up of a cooked fruit flavored muffin. An image of some sort of strawberry flaky pastery on display',  100)\n",
      " ('The sign is in clear view by the traffic lights. A Juice Theory sign is near a red traffic light. The bright sign is hanging next to the traffic signal. A sign on a electrical post that is about juice. A sign reading \"Juice Theory\" near a traffic light.', 1000)\n",
      " ('A cat is  sleeping inside a duffel bag. An orange cat lays in a black duffel bag. A cat sitting inside of a bag by a curtain. a small dog is packes into a duffel bag a travel bag with a cat laying inside of it', 1001)\n",
      " ('Two men walking through a field next to a large jet liner. Two people walking through a field as a plane lands. people walking down a path as a plane lands on a runway Two men are standing in a field near a landing strip. Two men walking a dog and watching an airplane about to take off.', 1002)\n",
      " ('A group of people riding skis down a snow covered street. A family skiing a city street while others clean snow off their cars. People are riding on skis in the snow on a street. Several people going down a snowy street in skis. The people have there skis on in the middle of the street.', 1003)\n",
      " ('A couple of guys standing up and playing Wii together. Adult men actively play video games with Wii controllers. A group of young men are playing a video game. Two men standing in a living room holding Wii remotes and nun-chucks. A couple of men standing in a living room together.', 1004)\n",
      " ('The boats navigated around the curves of the attraction. A few empty boats at a river ride Small boats sit unused in water by a dock. Different color boats along a river in a water park. A multicolored boat ride at an amusement park.', 1005)]\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_data.shape, raw_test_data.shape)\n",
    "print(raw_train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The skateboarder is putting on a show using the picnic table as his stage. A skateboarder pulling tricks on top of a picnic table. A man riding on a skateboard on top of a table. A skate boarder doing a trick on a picnic table. A person is riding a skateboard on a picnic table with a crowd watching.', 0)\n",
      " ('A bowl of soup that has some carrots, shrimp, and noodles in it. The healthy food is in the bowl and ready to eat. Soup has carrots and shrimp in it as it sits next to chopsticks. A tasty bowl of ramen is served for someone to enjoy. Bowl of Asian noodle soup, with shrimp and carrots.', 1)\n",
      " ('A man who is walking across the street. A busy intersection with an ice cream truck driving by. a man walks behind an ice cream truck A man is crossing a street near an icecream truck. The man is walking behind the concession bus.', 2)\n",
      " ('A young boy throwing a frisbee in a grassy field a young boy in the park throwing a frisbee A young boy throws a frisbee in a tree lined park. A kid in a city park throws a bright green Frisbee. A boy throwing a green frisbee in a grass field.', 3)\n",
      " ('A young child in the yard holding up a bat. A boy raring back with a baseball bat in a yard. A little boy has a baseball bat in a yard. A boy in black clothes holds a baseball bat over his shoulder near a fence and a tree. a little boy playing in his yard with a baseball bat', 4)\n",
      " ('A small bedroom with a desk and computer in it. A bedroom that has a desk, chair, and bed in it. A chair sitting in front of a brown desk. A desk beside a window in a bedroom a bed and a desk and chair by a big window', 5)\n",
      " ('A small child is eating a donut fed by another hand. A blonde boy looking at his donut hole. A boy is looking at a small doughnut with powdered sugar on his face. A little boy eating powered sugar covered flood. a child eats part of a sugar frosted donut', 6)\n",
      " ('a man is standing in the woods wearing a hat and glasses A man in sunglasses wears a grass tie and hat. A man wearing a woven crown and tie. a man in a blue shirt with some weird crown a man that is standing around in a hate', 7)\n",
      " ('this is a street with a brick building a building with a sign and a street right outside Black and white image of an empty street. a old building that is next to a street A black and white picture of a building', 8)\n",
      " ('A tent in a forest next to a silver and brown RV. a camp site and a trailer on the road Two pictures of a tent and a bus. Two juxtaposed pictures show a tent and a motor home. There is a tent with a photo of a bus under it.', 9)]\n"
     ]
    }
   ],
   "source": [
    "raw_train_data.sort(order = 'label', axis = 0)\n",
    "raw_test_data.sort(order = 'label', axis = 0)\n",
    "print(raw_train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keep_pos_nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "keep_pos_all = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBG', 'VBZ']\n",
    "keep_pos_verbs = ['VB', 'VBG', 'VBZ']\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(data, pos_to_keep = None, lemma = False):\n",
    "    \n",
    "    out = data.copy()\n",
    "    out['sentence'] = list(map(lambda x:x.lower(), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:''.join(ch for ch in x if ch not in punctuation), out['sentence']))\n",
    "    out['sentence'] = list(map(lambda x:' '.join(w for w in x.split(' ') if w not in stop_words), out['sentence']))\n",
    "    if lemma == True:\n",
    "        out['sentence'] = list(map(lambda x:' '.join(wnl.lemmatize(w) for w in x.split(' ')), out['sentence']))\n",
    "    \n",
    "    if pos_to_keep != None:\n",
    "        out['sentence'] = list(map(lambda x:' '.join(w[0] for w in nltk.pos_tag(nltk.word_tokenize(x)) if w[1] in pos_to_keep), out['sentence']))\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(raw_train_data, keep_pos_all, lemma = True)\n",
    "test_data =  preprocess(raw_test_data, keep_pos_all, lemma = True)\n",
    "\n",
    "train_data_nouns = preprocess(raw_train_data, keep_pos_nouns, lemma = True)\n",
    "test_data_nouns =  preprocess(raw_test_data, keep_pos_nouns, lemma = True)\n",
    "\n",
    "train_data_verbs = preprocess(raw_train_data, keep_pos_verbs, lemma = True)\n",
    "test_data_verbs =  preprocess(raw_test_data, keep_pos_verbs, lemma = True)\n",
    "\n",
    "\n",
    "train_data_noLemma = preprocess(raw_train_data, keep_pos_all)\n",
    "test_data_noLemma =  preprocess(raw_test_data, keep_pos_all)\n",
    "\n",
    "train_data_nouns_noLemma = preprocess(raw_train_data, keep_pos_nouns)\n",
    "test_data_nouns_noLemma =  preprocess(raw_test_data, keep_pos_nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('skateboarder putting show using picnic table stage skateboarder pulling trick top picnic table man riding skateboard top table skate boarder trick picnic table person riding skateboard picnic table crowd watching', 0)\n",
      " ('bowl soup carrot shrimp noodle healthy food bowl ready eat soup carrot shrimp sits chopstick tasty bowl ramen someone enjoy bowl asian noodle soup shrimp carrot', 1)\n",
      " ('man walking street busy intersection ice cream truck driving man walk ice cream truck man crossing street icecream truck man walking concession bus', 2)\n",
      " ('young boy throwing frisbee grassy field young boy park throwing frisbee young boy throw tree park kid city park throw green frisbee boy throwing green frisbee grass field', 3)\n",
      " ('young child holding bat boy raring baseball bat little boy baseball bat boy black clothes baseball bat shoulder fence tree little boy playing baseball bat', 4)\n",
      " ('small bedroom desk computer bedroom desk chair chair sitting front brown desk desk window bedroom desk chair big window', 5)\n",
      " ('small child eating donut hand blonde boy looking donut hole boy looking small doughnut sugar face little boy eating sugar flood child eats part sugar donut', 6)\n",
      " ('man standing wood wearing glass man sunglass wear grass tie man wearing woven crown tie man blue shirt weird crown man standing hate', 7)\n",
      " ('street brick building building sign street right outside black white image empty street old building next street black white picture building', 8)\n",
      " ('tent next silver brown rv camp site trailer road picture tent bus juxtaposed picture show tent motor home tent photo bus', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (2000,)\n",
      "[('skateboarder putting show using picnic table stage skateboarder pulling tricks top picnic table man riding skateboard top table skate boarder trick picnic table person riding skateboard picnic table crowd watching', 0)\n",
      " ('bowl soup carrots noodles healthy food bowl ready eat soup carrots sits chopsticks tasty bowl ramen someone enjoy bowl asian noodle soup shrimp carrots', 1)\n",
      " ('man walking street busy intersection ice cream truck driving man walks ice cream truck man crossing street icecream truck man walking concession bus', 2)\n",
      " ('young boy throwing frisbee grassy field young boy park throwing frisbee young boy throws frisbee tree park kid city park throws bright green frisbee boy throwing green frisbee grass field', 3)\n",
      " ('young child holding bat boy raring baseball bat little boy baseball bat boy black clothes holds baseball bat shoulder fence tree little boy playing baseball bat', 4)\n",
      " ('small bedroom desk computer bedroom desk chair chair sitting front brown desk desk window bedroom desk chair big window', 5)\n",
      " ('small child eating donut hand blonde boy looking donut hole boy looking small doughnut sugar face little boy eating sugar flood child eats part sugar donut', 6)\n",
      " ('man standing woods wearing hat glasses man sunglasses wears grass tie man wearing woven crown tie man blue shirt weird crown man standing hate', 7)\n",
      " ('street brick building building sign street right outside black white image empty street old building next street black white picture building', 8)\n",
      " ('tent next silver brown rv camp site trailer road pictures tent bus pictures tent motor home tent photo bus', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_noLemma.shape, test_data_noLemma.shape)\n",
    "print(train_data_noLemma[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = set()\n",
    "for s in train_data['sentence']:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        word_dict.add(w)\n",
    "\n",
    "noun_dict = set()\n",
    "for s in train_data_nouns['sentence']:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        noun_dict.add(w)\n",
    "        \n",
    "verb_dict = set()\n",
    "for s in train_data_verbs['sentence']:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        verb_dict.add(w)\n",
    "        \n",
    "word_noLemma_dict = set()\n",
    "for s in train_data_noLemma['sentence']:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        word_noLemma_dict.add(w)\n",
    "\n",
    "noun_noLemma_dict = set()\n",
    "for s in train_data_nouns_noLemma['sentence']:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        noun_noLemma_dict.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7038\n",
      "4832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'abandoned',\n",
       " 'abdomen',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abll',\n",
       " 'abnormal',\n",
       " 'aboard',\n",
       " 'abook',\n",
       " 'abou',\n",
       " 'abraham',\n",
       " 'abreast',\n",
       " 'abstract',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abyss',\n",
       " 'acacia',\n",
       " 'academic',\n",
       " 'accelerates',\n",
       " 'accent']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_dict))\n",
    "print(len(noun_dict))\n",
    "word_dict_list = list(word_dict)\n",
    "word_dict_list.sort()\n",
    "word_dict_list[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8339\n",
      "5924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'abandoned',\n",
       " 'abdomen',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abll',\n",
       " 'abnormal',\n",
       " 'aboard',\n",
       " 'abook',\n",
       " 'abou',\n",
       " 'abraham',\n",
       " 'abreast',\n",
       " 'abstract',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'acacia',\n",
       " 'academic',\n",
       " 'accelerates',\n",
       " 'accent']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(word_noLemma_dict))\n",
    "print(len(noun_noLemma_dict))\n",
    "word_noLemma_dict = list(word_noLemma_dict)\n",
    "word_noLemma_dict.sort()\n",
    "word_noLemma_dict[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accelerates',\n",
       " 'accenting',\n",
       " 'accompanying',\n",
       " 'accumulating',\n",
       " 'acknowledging',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'adjoining',\n",
       " 'adjusting',\n",
       " 'adjusts',\n",
       " 'admiring',\n",
       " 'adult',\n",
       " 'advertises',\n",
       " 'advertising',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'aging',\n",
       " 'aiming',\n",
       " 'airplane',\n",
       " 'airstrip']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(verb_dict))\n",
    "verb_dict = list(verb_dict)\n",
    "verb_dict.sort()\n",
    "verb_dict[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_data, test_data, vocab, binary = False):\n",
    "\n",
    "    Tfidf = TfidfVectorizer(vocabulary = vocab, binary = binary,\n",
    "                           tokenizer = lambda str: str.split(\" \"),)\n",
    "    tr_d = [word for word in train_data['sentence'].tolist()]\n",
    "    te_d = [word for word in test_data['sentence'].tolist()]\n",
    "\n",
    "    X_train = Tfidf.fit_transform(tr_d)\n",
    "    X_test = Tfidf.fit_transform(te_d)\n",
    "\n",
    "    return(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7038) (2000, 7038)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = vectorize(train_data, test_data, word_dict)\n",
    "X_train_nouns, X_test_nouns = vectorize(train_data_nouns, test_data_nouns, noun_dict)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_descriptions\"):\n",
    "    os.mkdir(\"data/processed_descriptions\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions.npz\", X_train)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions.npz\", X_test)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_nouns.npz\", X_train_nouns)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_nouns.npz\", X_test_nouns)\n",
    "\n",
    "with open('data/processed_descriptions/word_list.txt', 'w') as f:\n",
    "    for item in word_dict_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary, X_test_binary = vectorize(train_data, test_data, word_dict,binary=True)\n",
    "X_train_nouns_binary, X_test_nouns_binary = vectorize(train_data_nouns, test_data_nouns, noun_dict,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_descriptions\"):\n",
    "    os.mkdir(\"data/processed_descriptions\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_binary.npz\", X_train_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_binary.npz\", X_test_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_nouns_binary.npz\", X_train_nouns_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_nouns_binary.npz\", X_test_nouns_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8339) (2000, 8339)\n"
     ]
    }
   ],
   "source": [
    "X_train_noLemma, X_test_noLemma = vectorize(train_data_noLemma, test_data_noLemma, word_noLemma_dict)\n",
    "X_train_nouns_noLemma, X_test_nouns_noLemma = vectorize(train_data_nouns_noLemma, test_data_nouns_noLemma, noun_noLemma_dict)\n",
    "print(X_train_noLemma.shape, X_test_noLemma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_descriptions\"):\n",
    "    os.mkdir(\"data/processed_descriptions\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_noLemma.npz\", X_train_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_noLemma.npz\", X_test_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_nouns_noLemma.npz\", X_train_nouns_noLemma)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_nouns_noLemma.npz\", X_test_nouns_noLemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_noLemma_binary, X_test_noLemma_binary = vectorize(train_data_noLemma, test_data, word_noLemma_dict,binary=True)\n",
    "X_train_nouns_noLemma_binary, X_test_nouns_noLemma_binary = vectorize(train_data_nouns_noLemma, test_data_nouns_noLemma, noun_noLemma_dict,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/processed_descriptions\"):\n",
    "    os.mkdir(\"data/processed_descriptions\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_noLemma_binary.npz\", X_train_noLemma_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_noLemma_binary.npz\", X_test_noLemma_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_nouns_noLemma_binary.npz\", X_train_nouns_noLemma_binary)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_nouns_noLemma_binary.npz\", X_test_nouns_noLemma_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_verbs, X_test_verbs = vectorize(train_data, test_data, verb_dict)\n",
    "if not os.path.isdir(\"data/processed_descriptions\"):\n",
    "    os.mkdir(\"data/processed_descriptions\")\n",
    "\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/train_descriptions_verbs.npz\", X_train_verbs)\n",
    "scipy.sparse.save_npz(\"data/processed_descriptions/test_descriptions_verbs.npz\", X_test_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
